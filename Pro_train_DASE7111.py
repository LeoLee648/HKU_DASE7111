# train.py
import os
import argparse
from pathlib import Path
from rl4co.envs.routing import TSPEnv, TSPGenerator
from rl4co.models import AttentionModelPolicy, POMO
from rl4co.utils import RL4COTrainer
from lightning.pytorch.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import ModelCheckpoint

def main():
    parser = argparse.ArgumentParser(description="Train TSP model with configurable hyperparameters")
    
    # è¶…å‚æ•°
    parser.add_argument("--epochs", type=int, default=10, help="Number of training epochs")
    parser.add_argument("--num_loc", type=int, default=50, help="Number of cities in TSP")
    parser.add_argument("--lr", type=float, default=1e-4, help="Learning rate")
    parser.add_argument("--batch_size", type=int, default=64, help="Batch size")
    parser.add_argument("--seed", type=int, default=12345, help="Random seed")
    parser.add_argument("--gpu", action="store_true", help="Use GPU if available")
    
    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­ï¼ˆå¯é€‰ä½†æ¨èï¼‰
    import torch, numpy as np, random
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)

    # æ„å»ºæœ‰æ„ä¹‰çš„ version åç§°ï¼ˆå…³é”®ï¼ï¼‰
    version_name = f"epochs{args.epochs}_numloc{args.num_loc}_lr{args.lr}_bs{args.batch_size}_seed{args.seed}"
    print(f"ğŸš€ Starting experiment: {version_name}")

    # åˆ›å»ºç¯å¢ƒå’Œæ¨¡å‹
    generator = TSPGenerator(num_loc=args.num_loc, loc_distribution="uniform")
    env = TSPEnv(generator)
    policy = AttentionModelPolicy(env_name=env.name, num_encoder_layers=6)
    model = POMO(
        env, 
        policy, 
        batch_size=args.batch_size, 
        optimizer_kwargs={"lr": args.lr}
    )

    # Logger å’Œ Checkpoint
    logger = TensorBoardLogger(
        save_dir="lightning_logs",
        name="tsp_pomo",
        version=version_name  # â† è‡ªåŠ¨åˆ›å»ºå¸¦å‚æ•°çš„ç›®å½•ï¼
    )

    checkpoint_callback = ModelCheckpoint(
        monitor="val/reward",
        mode="max",
        save_top_k=1,
        save_last=True,
        filename="best-{epoch}-{step}-{val/reward:.4f}",
        verbose=True,
    )

    # Trainer
    trainer = RL4COTrainer(
        max_epochs=args.epochs,
        accelerator="gpu" if args.gpu else "cpu",
        precision="16-mixed" if args.gpu else "32-true",
        logger=logger,
        callbacks=[checkpoint_callback],
        log_every_n_steps=50,
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.fit(model)

if __name__ == "__main__":
    main()